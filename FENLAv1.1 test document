\begin{document}

\chapter{多元统计分析}

\section{概述}

《多元统计分析》的初等版本主要考虑：对多元正态rv进行分析。具体包括：

\begin{enumerate}
    \item 『置信区间暨假设检验』：解某个特定概率测度（以下简称为rv）的定积分方程，获得其分布函数的反函数。（用统计人士更熟悉的话说：求分布函数在y=0.05、0.95时的x的值。）
    
    \item 『主成分分析』：分解$(L^2)^p$中rv的内积矩阵，获得其正交相似标准形）。
    
    \item 『因子分析』：考虑一种参数模型，即用已知性不同的随机变量和确定变量建立的结构方程。在此参数模型下，解出最优参数。具体地说，解一部分随机变量与一些确定变量的函数关系，并使之满足一定限制条件（例如损失函数）。
    
    \item 『典型相关分析』：『主成分分析』的变形。
    
    \item 『判别分析』：基于$\RR^n$中的两类点，计算半正定距离函数$f_+$的支撑集$D$的边界$\pds D$。
    
    \item 『聚类分析』：基于$\RR^n$中的一些点，计算损失函数最小的分类数及分类方法（往往还计算『判别分析』中的边界$\pds D$。）
    
\end{enumerate}

当然，以上这些都是方法的数学表示。下面有一些基于统计学自己的理论体系的总结：

\begin{enumerate}
    \item 『置信区间暨假设检验』：通过一些随机变量和一些确定变量（称为参数）的函数关系，了解这些rv形成的什么样的区间最可能包括参数的给定值，从而可以认为这个给定值在或不在这个区间内。
    
    \item 『主成分分析』：寻求多元rv中使方差最大的成分。
    
    \item 『因子分析』：稍复杂一些的『置信区间暨假设检验』。
    
    \item 『典型相关分析』：寻求多元rv中使相关系数最大的成分。
    
    \item 『判别分析』：考虑已经预分类的两类点，计算哪一些区域离一类点的距离较另一类点更近。
    
    \item 『聚类分析』：考虑没有预分类的一些点，计算如何分类更合适。
    
\end{enumerate}

\section{推断期望}

\xiaoj{背景}

期望是rv的积分值/$L^1$意义下的大小，是最重要的特征数。所以，在推断一个类型的rv时，首先推断期望。

重新回顾参数统计意义下的统计推断，是指：

假设有一些指标变量（也是确定变量）$i\sy\Ih$给一族概率测度$\PP_i$编号，在这样的情况下，我们需要通过这些$\PP_i$间的差别来解出，对于一个rv $X$的每一个取值而言，$X$更倾向于服从哪一个$\PP_i$。

其具体手段往往是构造$F(X,i)=0$（服从0分布的随机变量，下同），然后分析这个函数$F$（称为枢轴量）。

当然，除了这种精确的办法，也可以用极限的办法。在实际采样中经常考虑某种特定的rv（iid）的特定的极限（譬如：极限变量为采样次数n，极限函数为样本矩），而格里文科定理确保即便你只是无脑地采用了这种特定的package pattern，也能得到在极限意义上差不多的结果。这样就能简化统计工作。

曾经有一名科研人员说：第一个人研究在木头桌子上雕花，第二个人研究所需的工时，等到第五个人就开始研究怎么在崖壁的某块特定的石头上雕花，并声称全世界有很多这类的石头（以使自己的研究看起来还算保留了一点泛用性）。以上的这种package pattern（iid变量，极限变量为采样次数n，极限函数为样本矩，不考虑采样次序问题）显然就是这类降低了泛化性、针对某种特殊情况的研究。但是了解那些可能存在而事实上未必存在的研究，即了解研究的全局，有利于在知识之间建立联系。在其它的统计学课程中，还存在着其它的package pattern。

\xiaoj{回顾一元统计分析}

有了前面的铺垫，我们得以在这里直接考虑正态iid样本的均值$\bar{x}$。将它与均值的某个待定假设$\miu_0$进行平均，然后再平均掉$\bar{x}$给它带来的波动性（这是为了让结果更具可比性；只清空x前两阶的影响是因为CLT及正态分布只要求前两阶）。

因为『参数估计』的基本规律（必须使用随机变量表述确定变量），第二阶矩必然要进行估计。尽管这估计破坏了正态性，但不算是不准的，可以用$\delta$方法（Taylor公式）打个哈哈就过去了。这样一来，情况还是原来的正态性。但是某些统计学家（Fisher）认为这个误差确实会造成一定的影响，所以计算出其精准的分布，t分布。

t统计量通用的CI公式为：均值$\fz t$临界值$\cha$标准误。CI以外就是拒绝域了。

\xiaoj{多元情况}

在多元而初等的场合中，最主要的思路是转化为一元。一个向量的二次型，就是它转化成一元数字的主要方式，所以本课程涉及很多二次型。一般，引导矩阵不为$E$的二次型都可以通过$x\yingsjt Ax$转化为引导矩阵为$E$的情况，所以二次型又简化为了内积。而分数又转化为了矩阵的逆。这样就不难想象，多元中也有一个T分布。不同的是，由于一元中t分布是根号出来的，所以为了简便，多元中就不加根号了（回顾向量范数和矩阵平方根的定义）。

有一个不那么显然的巧合：$T^2$就是F分布。

抽象地说，$T^2$分布也可以说是由仅中心化的正态分布，以及W分布生成的。考虑到，设$x$是向量，$x^T x$和$xx^T$都可以反应$x$的大小，此时前者可以说是$\kai^2$分布，后者则是W分布。

这里我们需要一个（非MLE法的）归一化常数n。从t的构造上看，是n而非n-1是因为这是一个无偏倚估计。MLE是基于另一套哲学，哲学不同，目的也就不同。

性质：$T^2$是仿射不变的。即若$x\yingsjt y=Ax+b$则针对x与y的$T^2$相同。推导是很容易的，这类只涉及基本矩阵代数的推导就省略了。

性质：$T^2$（在等价检验量意义下）是似然比检验。

统计量是指满足以下性质的函数：一、从rv映上rv，记为$X\yingsjt Y$；二、当给定模型中所有样本点$\omega$、使因变量为退化rv $Y(\omega)$时，存在确定的与因变量同维的数c，使得$Y(\omega)$几乎必然对所有感兴趣参数的笛卡尔积不变且均$=c$。

两个检验量的等价：给定一个具体的检验，这包括了模型假设、感兴趣参数、$H_0$、$H_1$四个方面。此时任何一个统计量都可以人为指派为一个检验量（只是功效不同；一般需要满足一些基础条件）。如果对于任意的显著性水平$\alpha$，两个检验量的拒绝域都一一映射，那么称两个检验量是等价的。特别地：如果$f$是严格单调函数，$S$是检验量，那么$T\diand f(S)$也是检验量且与S等价。

LRT的详细内容超出本科范围，这里略过。特别提到：从数学角度看待LRT时，可以看出似然比检验在缩小参数范围时，必然要求至少以降低1个维度的程度进行缩小。因为其中的$\kai^2$的自由度为$dim\daomega-dim\daomega_0 \dd 1$。

\xiaoj{置信区间的形状}

置信区间是指由rv构造出的区间，使得rv有95％概率（或者其它）包含确定参数。和前面提到的一些事一样，这也来源于实际应用中『参数未知而样本rv已知』的背景。

按照之前的方式对t统计量自然地进行推广，得到的『置信区间』并不是一个多维区间。设想最简单的情况：$x^2\xd r^2$，这样得到的是一个圆。如果二次型的引导矩阵不是$E$，那么就相当于做一次仿射变换，变成椭圆。为了避免混淆，称之为置信域CR。

也有得到传统意义上的区间的方法。

Bonferroni的方法最简单：假设多元随机变量$x$的各维$x_1$到$x_p$是互相独立的（否则，由概率的可数次可加性得到一样的推导）。那么就可以进行边际化，认为显著性水平以外的5％实际上是p个$\fen{5}{p}$。对每一维都做一个$\fen{5}{p}$显著性的检验，然后强制并在一起，就得到了真正的置信区间。

更进一步，可以认为$x_1$是$x$的一个线性组合。这个看法在『实验设计』中也遇到过，可以帮助我们处理更多的情况，比如$x_1+x_2$。从数学的角度，这相当于做了一次可逆Jacobi变换。这在数学分析中是容易理解的。

由此，设线性组合的系数是$a$，甚至可以找到对所有a{\ys{purple}一致地}成立的置信区间。实践证明这不会导致$+\wuq$。先固定a，进行和之前一样的合适步骤，然后以置信区间的半径作为目标函数，对a取极大（越大，信息量越少，越泛化），最后刚好归纳到了$T^2$。期间需要使用二次型之比的线性代数结论和内积不等式。

\xiaoj{大样本估计}

根据似然比原则，前述$T^2$统计量有2个渐近估计：$\kai^2$和正态分布。它们之间哪一个更精确，我也不知道（笑）。

前面的『线性组合CI』也用到了$T^2$，而因为上一自然段的内容是依分布的（而不管这个$T^2$是哪里来），所以也同样可以代入。

下面总结了一个可以代入的表格。

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{大样本估计中分布的约等关系}
    \begin{tabular}{llll}
    大样本时是否可以代入 & 有无$T^2$ & $\kai^2$ & 正态 \\
    传统（椭圆）置信域 & 有     & 可以代入  & 可以代入 \\
    方形Bonferroni置信区间 & 无     & 不可代入  & 可以代入 \\
    方形线性组合一致置信区间 & 有     & 可以代入  & 可以代入 \\
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%

注意：所谓『渐近』是指$\fen{1}{n}=o(\fen{1}{p})$。根据高阶无穷小定义，n必须是p的很多倍。在p很大时需要引起重视。为有说服力，在数学和统计经验之间，至少要适合一边的道理。

\xiaoj{成对总体均值的关系}

刚刚分析的都是一个参数的大小。换言之，是一个参数和一个常数的关系。接下来要分析的是两个参数的关系。

即便是前几节的『一个参数』，也是指一族的参数，即多个参数。那么如果出现两族参数，是否可以直接把它们首尾相接合并在一起（这样最简单）呢？是可以的。不过，请读者考虑以下2个问题的本质差别：

\begin{enumerate}
    \item $\miu_1=0;\miu_2=1$
    
    \item $\miu_1=\miu_2$
\end{enumerate}

从数学角度看，问题并不是『两个参数』或者『两族』、『两群』什么的。问题在于第二组假设中没有常数。更进一步地提炼，发现第二组假设是不完全的。倘若是一个完全的线性假设，有p个那么多（p是参数个数），那么与第一组就毫无不同。所以尽管本小节命名为『总体均值的关系』以便于理解，可是本小节方法的实质，是处理『均值的不完全关系』。

本小节是『均值的不完全关系』的第一部分，总结在成对情况下均值的不完全关系。

『成对』是一个应用概念，是对特定场合的进一步的优化（与『正则化』/『泛化』相对）。某些时候，一些统计学家相信即便是误差中也存在着少量的信息，所以有意让同一组的两次采样被绑定在一起。或者说，{\ys{purple}只研究它们的差}。这就是『成对』。『成对』是一个追加的假设，所以带成对条件的模型和非成对模型就不是同一个模型了。

通过这样的处理，『成对』情况下某些特定的『均值不完全关系』转化为了单总体的均值完全关系，仿照之前几节就可以解决它。

然而，即便成对，某些均值不完全关系也不能转化为均值完全关系。假设甲、乙是两个人（占第一指标位），在1d和3d（占第二指标位）分别采样2次。记均值为$\miu_{ij}$。但是我们关心：

\[
\miu_{11} + \miu_{22} = 0.
\]

一共有4个毫无关系的参数，只有1个要求，是不完全参数关系。然而这不能算是『成对』的参数假设。如果没有其他条件，一般无法作差转化为均值完全关系。

（至于你问我为什么要关心这个，我也不知道... ...纯属数学考虑。）

（硬编一个吧。比如甲在d-1发病，乙在d0发病，两个人那个『有可比性』的数据本来就差一天。观测在3d后还将每2d继续下去。如果是这个情况的话，还可以再做可逆Jacobi变换，把数据变回来，什么的。）

作差后，问题的数学本质已经得以转换，可以完全沿用前几段的分析方法。

\ct{推广：}

根据上文的分析精神，尤其是成对分析的方法，现在也可以分析『成三』、『成四』等特殊假设下的多维rv的均值。作差方式可以自愿制定，并表述成矩阵的形式。具体地说，设均值为$\miu$，可以分析$C\miu, C\sy\Mh(dim \hezi{假设}, dim \miu)$的情况。

\xiaoj{非成对多总体均值的关系}

和上一小节一样，仍然分析『不完全参数假设』，且参数为均值的问题。首先假设只有2个总体，且方差相等。因为非成对，一般假设两个总体的样本数不同。

回忆上一节第一个问题：能否直接把两个样本首尾相接合并在一起，形成一个新的样本，然后再考虑一个矩阵$C$左乘新的样本$\biyin{x}$？会面临第一个问题：两段样本的均值不相等。没关系，直接分别中心化。既然是同方差（否则，会遇到困难的Behrens-Fisher问题，和一元一样），那还能有什么不相等？

至于怎么取C，举个例子：

\[
\biyin{x}=(x_{11},x_{12},x_{21},x_{22})
\]

假设为：

\hj{\mu_1=\mu_2}

矩阵为：

\hj{C=\jz{1&0\\ 0&1\\ -1&0\\ 0&-1}}

完全是可以直接分析$C\biyin{x}=0$这个假设的。我也不知道为什么不这样...可能只是因为某些论文认为这种方法不够好吧。

暂时跳过。

\xiaoj{F检验/多元方差分析}

方差分析是老生常谈了。实际上方差分析就是带建模的F检验。方差分析属于『不完全参数假设』且参数为均值的一种情况。它的核心思想就是首先用F检验量把总体的差异汇聚到一起，如果得出总体有差异的结论，再（用上文的方法）分开检测。这样做，性价比比较高，对一些显著性临界的情况有更高的灵活性和可解释度。

在一元方差分析中，F检验等价于t检验，并且一般的『假设检验』就直接是『方差分析』的结论了。多元方差分析按道理也可以这样做（$T^2$开个根号就是T，乘以一个常数就是F，已经这样做了）。然而讲义上采取了一种变种的做法。

它将平方和与检验量的生成都由$x^Tx$模式改为了$xx^T$模式。这样一来，平方和都变成了矩阵，检验量（的前体）也变成了矩阵。为了让检验能够得到唯一结论（大概也可以对矩阵的每一个元素都和『理想矩阵』进行比较，然后进行某种意义上的投票），采用了行列式表示矩阵的大小。该行列式（由LRT，实际上是行列式之比）也成为了最终的检验量。

多因子的多元AOV也是一样的。

虽然本来还有一些（从数学角度）需要特别关注的内容，譬如『可加模型』和『乘法模型』甚至『核函数模型』，但这些都可以归纳到一元AOV上去，就省略了。

\xiaoj{形状分析}

之前几小节已经从数理的角度抽象和泛化了多元统计分析的主要研究目标，现在进行一个整理。

对于一个多元rv $x$，主要关心的三个目标就是：$x$；$a^Tx$；$Cx$。如果rv不止一个，有$q$个，那么还可以关心这$q$个rv的关系。此外还有前三个目标和最后一个目标的协同作用。在关心这些目标的时候，尤其注意（也是前文中唯一注意）的，就是目标的均值参数。

『形状分析』其实就是这四个目标中一种比较特殊的情形：分析一族均值函数之间的关系。它一共涉及到三维：形成函数的变量、比较的变量、每一个点代表的$p$维多元正态rv内部的$p$个变量。

还好，与前文内容相比，形状分析在数学上没有本质的变化。能分析几个维度，完全取决于能掌握几阶张量及其运算性质。不过，三阶就足够难了。如何定义三阶张量的乘法？这是一个问题。也许这就是前文有时不使用矩阵的原因之一（如果在前文有两个维度的情况下只使用矩阵，现在多一个维度，就不得不使用3阶张量了）。

\section{主成分分析}

\xiaoj{生成模型和样本模型}

像确定变量所形成的方程一样，随机变量也能形成十分有用的方程/方程组，这被称为『结构方程』（或者叫做模型）。它能揭示不同随机变量和确定变量之间形成的关系。和一般的方程一样，结构方程也会面临冗余与否的问题。有时1个结构方程便能确定大多数所涉事物，譬如对实随机变量$X,Y$，$X^2+Y^2=0, a.e.$确定了X和Y几乎处处为零。但有时给出的m个方程却只有m-1个有用。这方面的理论参见数学分析中隐函数的理论。

抛开统计学而进行泛化，一般的结构方程可以认为决定了其中事物的生成规律。就统计学而言，这个结构方程中一部分的rv是可以『样本化』（视作样本）的。这里的样本尤其指代iid的样本，当然在数学上也可以指代别的样本。这一系列的样本rv中的每一个都由这个结构方程（加上样本之间的相依关系）来生成。固然可以用每一个样本自己来『参数估计』，但是有时（譬如，在样本iid时），把各个样本（依据它们的相依/独立关系）汇总到一起来估计模型的参数，是好的选择。统计学出身的读者要意识到这一点在数理上并不显然，需要一些条件。一种特殊的情况见下一自然段。

这些样本源自的这些结构方程中，参数的情况还没有被讨论。很多时候参数是通用在这些结构方程里的，而方程的结构（或者说，是函数参数）也是通用的。但这同样不显然。在一些复杂的情况下，不同模型所来源的参数也会发生变化。更进一步，甚至这些参数可能是随机的。倘若是这样的话（参见bayes统计理论），从『估计参数』和『统计量』开始的数理分析，就要从『分析rv和确定变量的关系』走向『分析一部分rv和一部分rv』的关系了。即便参数是确定的，有时候不同样本背后的参数也会发生一些差异。一种特殊的情况是：倘若不同的样本享有完全不同的参数集（包括函数参数集，即，连模型也不同了），又另假设这些样本之间也是独立的，那么把这些样本放在一起考虑，就起不到什么作用。若设有N个样本，那么，还不如说是考虑N个不同的问题，每个问题都有1样本量。

总之：无论是参数的共用性还是不同样本之间的相依性，都能产生『视这些结构方程为一个整体』的直觉和必要性。如果上两条都失去了，那么这种整体法的看待就没有必要了。

接下来回到统计学，讨论一种实践上常用的情况：不同样本之间是独立的，并且模型的函数参数及其它参数的具体取值，对这些样本而言都是一致的（只是你不知道它取哪个常值）。参数的有效区间也是一致的。这种情况下，这些模型的通用模式（或者说，这些模型的任意一个），就叫做生成模型。如果能用这N个样本生成的统计量替换掉生成模型中的样本本体，那么这样的模型（已经与原模型不同了！）就叫做样本模型。

例如，设样本量为$N$，唯一的参数是$a$，生成模型是：

\[
Y_i=aX_i
\]

或

\[
Y=aX.
\]

用$\bar{Y}$代$Y$，用$\bar{X}$代$X$，得到的样本模型就会是：

\[
\bar{Y}=a\bar{X}.
\]

注意后一个模型所涉及的rv、所服从的分布都和前面的生成模型完全不同。但后一个模型有可能继承了前一个模型的部分性质。

\xiaoj{主成分分析的生成模型}

在阐明生成模型和样本模型的细微区别后，论述就变得容易多了。

设rv $x_{p\cha 1}~(*,\dasigma)$。考虑$\dasigma$的最大特征值对应的特征向量$a_1^T$。注意$a_1^Tx$也是x的使方差最大的线性组合（简称：成分）。逐个找下去，为了让这种查找不沦为显然，要求后一次与前所有次找到的向量正交。这就相当于完成了$\dasigma$的正交相似对角化。

性质：记$y_i$为$x$的第$i$个成分，那么$\fen{\lamda_i}{\sum_k \lambda_k}$为这个成分解释的方差的占比。

性质：特征向量的各个维度代表主成分中各个原始变量的权重。

性质：主成分分析会因为各个变量的量纲不同导致结果的不同、说服力的降低甚至结果显著偏离合理的范围。

\xiaoj{主成分分析的样本模型}

很简单，用矩估计就可以了。一旦明白样本模型是什么，就会发现很多统计方法的理解都变得轻松了。

\section{因子分析}


\begin{yxlb}
\item sdafd
\tiao dsasdas
\end{yxlb}






\end{document}
